# Tanslated-commands


### PyTorch
|                                              | PyTorch   | TensorFlow  | TensorFlow.js  |
|----------------------------------------------|-----------|-------------|--------------- |
| torch.utils.dlpack.from_dlpack               | ✔         | ✘          | ✘              |
| torch.utils.dlpack.to_dlpack                 | ✔         | ✘          | ✘              |
| torch.utils.model_zoo.load_url               | ✔         | ✘          | ✘              |
| torch.utils.cpp_extension.CppExtension       | ✔         | ✘          | ✘              |
| torch.utils.cpp_extension.CUDAExtension      | ✔         | ✘          | ✘              |
| torch.hub.load                               | ✔         | ✔          | ✘              |
| torch.utils.cpp_extension.load_inline        | ✔         | ✘          | ✘              |
| torch.utils.cpp_extension.include_paths      | ✔         | ✘          | ✘              |
| torch.utils.cpp_extension.check_compiler_abi_compatibility      | ✔         | ✘          | ✘              |
| torch.utils.cpp_extension.verify_ninja_availability             | ✔         | ✘          | ✘              |
| torch.distributed.rpc.get_worker_info        | ✔         | ✘          | ✘              |
| torch.utils.data.random_split                | ✔         | ✘          | ✘              |
| torch.utils.checkpoint.checkpoint            | ✔         | ✔          | ✘              |
| torch.utils.checkpoint.checkpoint_sequential | ✔         | ✘          | ✘              |
| torch.addmm                                  | ✔         | ✘          | ✘              |
| torch.mm                                     | ✔         | ✘          | ✘              |
| torch.sum                                    | ✔         | ✔          | ✔              |
| torch.distributed.rpc.init_rpc               | ✔         | ✘          | ✘              |
| torch.distributed.rpc.rpc_sync               | ✔         | ✘          | ✘              |
| torch.distributed.rpc.rpc_async              | ✔         | ✘          | ✘              |
| torch.distributed.rpc.remote                 | ✔         | ✘          | ✘              |
| torch.distributed.rpc.shutdown               | ✔         | ✘          | ✘              |
| torch.random.fork_rng                        | ✔         | ✘          | ✘              |
| torch.get_rng_state                          | ✔         | ✘          | ✘              |
| torch.initial_seed                           | ✔         | ✘          | ✘              |
| torch.manual_seed                            | ✔         | ✘          | ✘              |
| torch.seed                                   | ✔         | ✘          | ✘              |
| torch.set_rng_state                          | ✔         | ✘          | ✘              |
| torch.quantization.quantize                  | ✔         | ✔          | ✘              |
| torch.jit.export                             | ✔         | ✔          | ✘              |
| torch.nn.init.calculate_gain                 | ✔         | ✘          | ✘              |
| torch.nn.init.uniform_                       | ✔         | ✘          | ✘              |
| torch.nn.init.normal_                        | ✔         | ✘          | ✘              |
| torch.quantization.quantize_dynamic          | ✔         | ✘          | ✘              |
| torch.quantization.quantize_qat              | ✔         | ✘          | ✘              |
| torch.quantization.prepare                   | ✔         | ✘          | ✘              |
| torch.quantization.prepare_qat               | ✔         | ✘          | ✘              |
| torch.quantization.convert                   | ✔         | ✘          | ✘              |
| torch.quantization.add_quant_dequant         | ✔         | ✘          | ✘              |
| torch.quantization.add_observer_             | ✔         | ✘          | ✘              |
| torch.quantization.swap_module               | ✔         | ✘          | ✘              |
| torch.quantization.propagate_qconfig_        | ✔         | ✘          | ✘              |
| torch.quantization.default_eval_fn           | ✔         | ✘          | ✘              |
| torch.quantization.get_observer_dict         | ✔         | ✘          | ✘              |
| torch.nn.fucntional.relu                     | ✔         | ✔          | ✔              |
| linear                                       | ✔         | ✔          | ✘              |
| torch.t                                      | ✔         | ✔          | ✔              |
| torch.matmul                                 | ✔         | ✔          | ✔              |
| torch.nn.functional.conv2d                   | ✔         | ✔          | ✔              |
| torch.nn.functional.conv3d                   | ✔         | ✔          | ✔              |
| torch.nn.functional.max_pool2d               | ✔         | ✔          | ✘              |
| torch.nn.functional.adaptive_avg_pool2d      | ✔         | ✘          | ✘              |
| torch.nn.functional.avg_pool2d               | ✔         | ✔          | ✘              |
| torch.nn.functional.interpolate              | ✔         | ✘          | ✘              |
| torch.nn.functional.upsample                 | ✔         | ✘          | ✘              |
| torch.nn.functional.upsample_bilinear        | ✔         | ✘          | ✘              |
| torch.nn.functional.upsample_nearest         | ✔         | ✘          | ✘              |
| torch.onnx.register_custom_op_symbolic       | ✔         | ✘          | ✘              |
| torch.onnx.operators.functional.shape_as_tensor          | ✔         | ✘          | ✘              |
| torch.onnx.set_training                      | ✔         | ✘          | ✘              |
| torch.onnx.is_in_onnx_export                 | ✔         | ✘          | ✘              |
| torch.hub.list                               | ✔         | ✘          | ✘              |
| torch.hub.help                               | ✔         | ✘          | ✘              |
| torch.hub.download_url_to_file               | ✔         | ✘          | ✘              |
| torch.hub.load_state_dict_from_url           | ✔         | ✘          | ✘              |
| torch.hub.set_dir                            | ✔         | ✘          | ✘              |
| torch.cuda.is_initialized                    | ✔         | ✘          | ✘              |
| torch.nn.init.constant_                      | ✔         | ✘          | ✘              |
| torch.nn.init.ones_                          | ✔         | ✘          | ✘              |
| torch.nn.init.zeros_                         | ✔         | ✘          | ✘              |
| torch.nn.init.eye_                           | ✔         | ✘          | ✘              |
| torch.nn.init.dirac_                         | ✔         | ✘          | ✘              |
| torch.nn.init.xavier_uniform_                | ✔         | ✘          | ✘              |
| torch.nn.init.xavier_normal_                 | ✔         | ✘          | ✘              |
| torch.nn.init.kaiming_uniform_               | ✔         | ✘          | ✘              |
| torch.nn.init.kaiming_normal_                | ✔         | ✘          | ✘              |
| torch.nn.init.orthogonal_                    | ✔         | ✘          | ✘              |
| torch.nn.init.sparse_                        | ✔         | ✘          | ✘              |
| torch.cuda.current_blas_handle               | ✔         | ✘          | ✘              |
| torch.autograd.backward                      | ✔         | ✘          | ✘              |
| torch.distributed.is_mpi_available           | ✔         | ✘          | ✘              |
| torch.distributed.is_nccl_available          | ✔         | ✘          | ✘              |
| torch.jit.script                             | ✔         | ✘          | ✘              |
| torch.distribution.kl.kl_divergence          | ✔         | ✘          | ✘              |
| torch.cuda.current_device                    | ✔         | ✘          | ✘              |
| torch.cuda.current_stream                    | ✔         | ✘          | ✘              |
| torch.cuda.default_stream                    | ✔         | ✘          | ✘              |
| torch.cuda.device_count                      | ✔         | ✘          | ✘              |
| torch.cuda.get_device_capability             | ✔         | ✘          | ✘              |
| torch.cuda.get_device_name                   | ✔         | ✘          | ✘              |
| torch.cuda.init                              | ✔         | ✘          | ✘              |
| torch.cuda.ipc_collect                       | ✔         | ✘          | ✘              |
| torch.cuda.is_available                      | ✔         | ✘          | ✘              |
| torch.cuda.set_device                        | ✔         | ✘          | ✘              |
| torch.cuda.stream                            | ✔         | ✘          | ✘              |
| torch.cuda.synchronize                       | ✔         | ✘          | ✘              |
| torch.cuda.get_rng_state_all                 | ✔         | ✘          | ✘              |
| torch.cuda.set_rng_state_all                 | ✔         | ✘          | ✘              |
| torch.cuda.manual_seed_all                   | ✔         | ✘          | ✘              |
| torch.cuda.seed_all                          | ✔         | ✘          | ✘              |
| torch.cuda.comm.broadcast                    | ✔         | ✘          | ✘              |
| torch.cuda.comm.broadcast_coalesced          | ✔         | ✘          | ✘              |
| torch.cuda.comm.reduce_add                   | ✔         | ✘          | ✘              |
| torch.cuda.comm.scatter                      | ✔         | ✘          | ✘              |
| torch.cuda.seed_all                          | ✔         | ✘          | ✘              |
| torch.gather                                 | ✔         | ✔          | ✔              |
| torch.cuda.empty_cache                       | ✔         | ✘          | ✘              |
| torch.cuda.memory_stats                      | ✔         | ✘          | ✘              |
| torch.cuda.memory_summary                    | ✔         | ✘          | ✘              |
| torch.cuda.memory_snapshot                   | ✔         | ✘          | ✘              |
| torch.cuda.memory_allocated                  | ✔         | ✘          | ✘              |
| torch.cuda.max_memory_allocated              | ✔         | ✘          | ✘              |
| torch.cuda.reset_max_memory_allocated        | ✔         | ✘          | ✘              |
| torch.autograd.grad                          | ✔         | ✘          | ✔              |
| torch.autograd.gradcheck                     | ✔         | ✘          | ✘              |
| torch.autograd.gradgradcheck                 | ✔         | ✘          | ✘              |
| torch.autograd.profiler.load_nvprof          | ✔         | ✘          | ✘              |
| torch.trace                                  | ✔         | ✔          | ✘              |
| torch.jit.trace_module                       | ✔         | ✘          | ✘              |
| torch.jit.ignore                             | ✔         | ✘          | ✘              |
| torch.jit.unused                             | ✔         | ✘          | ✘              |
| torch.jit.is_scripting                       | ✔         | ✘          | ✘              |
| torch.distributions.kl.register_kl           | ✔         | ✘          | ✘              |
| torch.nn.functional.conv1d                   | ✔         | ✔          | ✔              |
| torch.cuda.memory_reserved                   | ✔         | ✘          | ✘              |
| torch.cuda.max_memory_reserved               | ✔         | ✘          | ✘              |
| torch.cuda.memory_cached                     | ✔         | ✘          | ✘              |
| torch.cuda.max_memory_cached                 | ✔         | ✘          | ✘              |
| torch.cuda.reset_max_memory_cached           | ✔         | ✘          | ✘              |
| torch.cuda.nvtx.mark                         | ✔         | ✘          | ✘              |
| torch.cuda.nvtx.range_push                   | ✔         | ✘          | ✘              |
| torch.cuda.nvtx.range_pop                    | ✔         | ✘          | ✘              |
| bernoulli_                                   | ✔         | ✘          | ✘              |
| torch.nn.fucntional.conv_transpose1d         | ✔         | ✘          | ✘              |
| torch.nn.fucntional.conv_transpose2d         | ✔         | ✘          | ✘              |
| torch.nn.fucntional.conv_transpose3d         | ✔         | ✘          | ✘              |
| torch.nn.fucntional.unfold                   | ✔         | ✘          | ✘              |
| torch.nn.fucntional.fold                     | ✔         | ✘          | ✘              |
| torch.nn.fucntional.avg_pool1d               | ✔         | ✔          | ✘              |
| torch.nn.fucntional.avg_pool3d               | ✔         | ✔          | ✘              |
| torch.nn.functional.max_pool1d               | ✔         | ✔          | ✘              |
| torch.nn.functional.max_pool3d               | ✔         | ✘          | ✘              |
| torch.nn.functional.max_unpool1d             | ✔         | ✘          | ✘              |
| torch.nn.functional.max_unpool2d             | ✔         | ✘          | ✘              |
| torch.nn.functional.max_unpool3d             | ✔         | ✘          | ✘              |
| torch.nn.functional.lp_pool1d                | ✔         | ✘          | ✘              |
| torch.nn.functional.lp_pool2d                | ✔         | ✘          | ✘              |
| torch.nn.functional.adaptive_max_pool1d      | ✔         | ✘          | ✘              |
| torch.nn.functional.adaptive_max_pool2d      | ✔         | ✘          | ✘              |
| torch.nn.functional.adaptive_max_pool3d      | ✔         | ✘          | ✘              |
| torch.nn.functional.adaptive_avg_pool1d      | ✔         | ✘          | ✘              |
| torch.nn.functional.adaptive_avg_pool3d      | ✔         | ✘          | ✘              |
| torch.nn.functional.threshold                | ✔         | ✘          | ✘              |
| torch.nn.functional.threshold_               | ✔         | ✘          | ✘              |
| torch.nn.functional.relu_                    | ✔         | ✘          | ✘              |
| to                                           | ✔         | ✘          | ✘              |
| all                                          | ✔         | ✔          | ✔              |
| any                                          | ✔         | ✔          | ✔              |
| torch.nn.functional.hardtanh                 | ✔         | ✘          | ✘              |
| torch.nn.functional.hardtanh_                | ✔         | ✘          | ✘              |
| torch.nn.functional.relu6                    | ✔         | ✔          | ✔              |
| torch.nn.functional.elu                      | ✔         | ✔          | ✔              |
| torch.nn.functional.elu_                     | ✔         | ✘          | ✘              |
| torch.nn.functional.selu                     | ✔         | ✔          | ✔              |
| torch.nn.functional.celu                     | ✔         | ✘          | ✘              |
| torch.nn.functional.leaky_relu               | ✔         | ✔          | ✘              |
| torch.nn.functional.leaky_relu_              | ✔         | ✘          | ✘              |
| torch.nn.functional.prelu                    | ✔         | ✔          | ✔              |
| torch.nn.functional.glu                      | ✔         | ✘          | ✘              |
| torch.nn.functional.gelu                     | ✔         | ✘          | ✘              |
| torch.nn.functional.logsigmoid               | ✔         | ✘          | ✔              |
| torch.nn.functional.hardshrink               | ✔         | ✘          | ✘              |
| torch.nn.functional.tanhshrink               | ✔         | ✘          | ✘              |
| torch.nn.functional.softsign                 | ✔         | ✔          | ✘              |
| torch.nn.functional.softplus                 | ✔         | ✔          | ✔              |
| torch.nn.functional.softmin                  | ✔         | ✘          | ✘              |
| torch.nn.functional.softmax                  | ✔         | ✔          | ✔              |
| torch.nn.functional.softshrink               | ✔         | ✘          | ✘              |
| torch.nn.functional.gumbel_softmax           | ✔         | ✘          | ✘              |
| torch.nn.functional.log_softmax              | ✔         | ✔          | ✘              |
| torch.tanh                                   | ✔         | ✔          | ✔              |
| torch.sigmoid                                | ✔         | ✔          | ✔              |
| torch.nn.functional.batch_norm               | ✔         | ✘          | ✘              |
| torch.nn.functional.instance_norm            | ✔         | ✘          | ✘              |
| torch.nn.functional.layer_norm               | ✔         | ✘          | ✘              |
| torch.nn.functional.local_response_norm      | ✔         | ✘          | ✘              |
| torch.nn.functional.normalize                | ✔         | ✔          | ✘              |
| torch.is_tensor                              | ✔         | ✔          | ✘              |
| torch.nn.functional.bilinear                 | ✔         | ✘          | ✘              |
| torch.nn.functional.alpha_dropout            | ✔         | ✔          | ✔              |
| torch.nn.functional.dropout                  | ✔         | ✘          | ✘              |
| torch.nn.functional.dropout2d                | ✔         | ✘          | ✘              |
| torch.nn.functional.dropout3d                | ✔         | ✘          | ✘              |
| torch.nn.functional.embedding                | ✔         | ✔          | ✔              |
| torch.nn.functional.embedding_bag            | ✔         | ✘          | ✘              |
| torch.nn.functional.one_hot                  | ✔         | ✔          | ✘              |
| torch.nn.functional.pairwise_distance        | ✔         | ✘          | ✘              |
| torch.nn.functional.cosine_similarity        | ✔         | ✔          | ✘              |
| torch.nn.functional.pdist                    | ✔         | ✘          | ✘              |
| torch.nn.functional.binary_cross_entropy     | ✔         | ✘          | ✘              |
| torch.nn.functional.binary_cross_entropy_with_logits     | ✔         | ✘          | ✘              |
| torch.nn.functional.poisson_nll_loss         | ✔         | ✘          | ✘              |
| torch.nn.functional.cosine_embedding_loss    | ✔         | ✘          | ✘              |
| torch.nn.functional.cross_entropy            | ✔         | ✘          | ✘              |
| torch.nn.functional.ctc_loss                 | ✔         | ✔          | ✘              |
| torch.nn.functional.hinge_embedding_loss     | ✔         | ✘          | ✘              |
| torch.nn.functional.kl_div                   | ✔         | ✘          | ✘              |
| torch.nn.functional.l1_loss                  | ✔         | ✘          | ✘              |
| torch.nn.functional.mse_loss                 | ✔         | ✘          | ✘              |
| torch.nn.functional.margin_ranking_loss      | ✔         | ✘          | ✘              |
| torch.nn.functional.multilabel_margin_loss   | ✔         | ✘          | ✘              |
| torch.nn.functional.multilabel_soft_margin_loss          | ✔         | ✘          | ✘              |
| torch.nn.functional.multi_margin_loss        | ✔         | ✘          | ✘              |
| torch.nn.functional.nll_loss                 | ✔         | ✘          | ✘              |
| torch.nn.functional.smooth_l1_loss           | ✔         | ✘          | ✘              |
| torch.nn.functional.soft_margin_loss         | ✔         | ✘          | ✘              |
| torch.nn.utils.clip_grad_norm_               | ✔         | ✘          | ✘              |
| torch.nn.utils.clip_grad_value_              | ✔         | ✘          | ✘              |
| torch.nn.utils.parameters_to_vector          | ✔         | ✘          | ✘              |
| torch.nn.utils.vector_to_parameters          | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.identity                | ✔         | ✔          | ✔              |
| torch.nn.utils.prune.random_unstructured     | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.l1_unstructured         | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.random_structured       | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.ln_structured           | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.global_unstructured     | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.custom_from_mask        | ✔         | ✘          | ✘              |
| torch.nn.utils.prune.remove                  | ✔         | ✔          | ✘              |
| torch.nn.utils.prune.is_pruned               | ✔         | ✘          | ✘              |
| torch.nn.utils.weight_norm                   | ✔         | ✘          | ✘              |
| torch.nn.utils.remove_weight_norm            | ✔         | ✘          | ✘              |
| torch.nn.utils.spectral_norm                 | ✔         | ✘          | ✘              |
| torch.nn.utils.remove_spectral_norm          | ✔         | ✘          | ✘              |
| torch.nn.utils.rnn.PackedSequence            | ✔         | ✘          | ✘              |
| torch.nn.utils.rnn.pack_padded_sequence      | ✔         | ✘          | ✘              |
| torch.nn.utils.rnn.pad_packed_sequence       | ✔         | ✘          | ✘              |
| torch.nn.utils.rnn.pad_sequence              | ✔         | ✘          | ✘              |
| torch.nn.utils.rnn.pack_sequence             | ✔         | ✘          | ✘              |
| torch.is_storage                             | ✔         | ✘          | ✘              |
| torch.is_floating_point                      | ✔         | ✘          | ✘              |
| torch.set_default_dtype                      | ✔         | ✘          | ✘              |
| torch.set_default_tensor_type                | ✔         | ✘          | ✘              |
| torch.numel                                  | ✔         | ✘          | ✘              |
| torch.set_printoptions                       | ✔         | ✘          | ✘              |
| torch.set_flush_denormal                     | ✔         | ✘          | ✘              |
| torch.tesnor                                 | ✔         | ✔          | ✔              |
| torch.sparse_coo_tensor                      | ✔         | ✘          | ✘              |
| torch.as_tensor                              | ✔         | ✘          | ✘              |
| torch.as_strided                             | ✔         | ✘          | ✘              |
| torch.from_numpy                             | ✔         | ✘          | ✘              |
| torch.zeros                                  | ✔         | ✔          | ✔              |
| torch.zeros_like                             | ✔         | ✔          | ✘              |
| torch.ones                                   | ✔         | ✔          | ✔              |
| torch.ones_like                              | ✔         | ✔          | ✘              |
| torch.range                                  | ✔         | ✔          | ✔              |
| torch.linspace                               | ✔         | ✔          | ✔              |
| torch.arrange                                | ✔         | ✔          | ✘              |
| torch.loagspace                              | ✔         | ✘          | ✘              |
| torch.eye                                    | ✔         | ✔          | ✔              |
| torch.empty                                  | ✔         | ✘          | ✘              |
| torch.nn.fucntional.triplet_margin_loss      | ✔         | ✘          | ✘              |
| torch.nn.functional.pixel_shuffle            | ✔         | ✘          | ✘              |
| torch.nn.functional.pad                      | ✔         | ✔          | ✔              |
| torch.nn.functional.grid_sample              | ✔         | ✘          | ✘              |
| torch.nn.fucntional.affine_grid              | ✔         | ✘          | ✘              |
| torch.nn.fucntional.data_parallel            | ✔         | ✘          | ✘              |
| torch.empty_strided                          | ✔         | ✘          | ✘              |
| torch.full                                   | ✔         | ✘          | ✘              |
| torch.full_like                              | ✔         | ✘          | ✘              |
| torch.quantize_per_tensor                    | ✔         | ✘          | ✘              |
| torch.quantize_per_channel                   | ✔         | ✘          | ✘              |
| torch.cat                                    | ✔         | ✘          | ✘              |
| torch.chunk                                  | ✔         | ✘          | ✘              |
| torch.index_select                           | ✔         | ✘          | ✘              |
| torch.masked_selet                           | ✔         | ✘          | ✘              |
| torch.narrow                                 | ✔         | ✘          | ✘              |
| torch.non_zero                               | ✔         | ✘          | ✘              |
| torch.reshape                                | ✔         | ✔          | ✔              |
| torch.split                                  | ✔         | ✔          | ✔              |
| torch.squeeze                                | ✔         | ✔          | ✔              |
| torch.stack                                  | ✔         | ✔          | ✔              |
| torch.t                                      | ✔         | ✘          | ✔              |
| torch.take                                   | ✔         | ✘          | ✔              |
| torch.transpose                              | ✔         | ✔          | ✔              |
| torch.unbind                                 | ✔         | ✘          | ✘              |
| torch.unsqueeze                              | ✔         | ✘          | ✘              |
| torch.where                                  | ✔         | ✔          | ✔              |
| torch.bernoulli                              | ✔         | ✘          | ✘              |
| torch.multinomial                            | ✔         | ✘          | ✔              |
| torch.normal                                 | ✔         | ✔          | ✘              |
| torch.rand                                   | ✔         | ✘          | ✘              |
| torch.rand_like                              | ✔         | ✘          | ✘              |
| torch.randint                                | ✔         | ✘          | ✘              |
| torch.randint_like                           | ✔         | ✘          | ✘              |
| torch.randn                                  | ✔         | ✘          | ✘              |
| torch.randn_like                             | ✔         | ✘          | ✘              |
| torch.randperm                               | ✔         | ✘          | ✘              |
| torch.get_num_threads                        | ✔         | ✘          | ✘              |
| torch.set_num_threads                        | ✔         | ✘          | ✘              |
| torch.get_num_interop_threads                | ✔         | ✘          | ✘              |
| torch.abs                                    | ✔         | ✔          | ✔              |
| torch.acos                                   | ✔         | ✔          | ✔              |
| torch.add                                    | ✔         | ✔          | ✔              |
| torch.addcdiv                                | ✔         | ✘          | ✘              |
| torch.addcmul                                | ✔         | ✘          | ✘              |
| torch.angle                                  | ✔         | ✔          | ✘              |
| torch.asin                                   | ✔         | ✔          | ✔              |
| torch.atan                                   | ✔         | ✔          | ✔              |
| torch.atan2                                  | ✔         | ✔          | ✔              |
| torch.bitwise_not                            | ✔         | ✘          | ✘              |
| torch.bitwise_xor                            | ✔         | ✔          | ✘              |
| torch.ceil                                   | ✔         | ✔          | ✔              |
| torch.clamp                                  | ✔         | ✘          | ✘              |
| torch.conj                                   | ✔         | ✔          | ✘              |
| torch.cos                                    | ✔         | ✔          | ✔              |
| torch.cosh                                   | ✔         | ✔          | ✔              |
| torch.div                                    | ✔         | ✘          | ✔              |
| torch.digamma                                | ✔         | ✔          | ✘              |
| torch.erf                                    | ✔         | ✔          | ✔              |
| torch.erfc                                   | ✔         | ✔          | ✘              |
| torch.erfinv                                 | ✔         | ✔          | ✘              |
| torch.exp                                    | ✔         | ✔          | ✔              |
| torch.expm1                                  | ✔         | ✔          | ✔              |
| torch.floor                                  | ✔         | ✔          | ✔              |
| torch.fmod                                   | ✔         | ✘          | ✘              |
| torch.frac                                   | ✔         | ✘          | ✘              |
| torch.imag                                   | ✔         | ✔          | ✔              |
| torch.lerp                                   | ✔         | ✘          | ✘              |
| torch.lgamma                                 | ✔         | ✔          | ✘              |
| torch.log                                    | ✔         | ✔          | ✔              |
| torch.log10                                  | ✔         | ✘          | ✘              |
| torch.log1p                                  | ✔         | ✔          | ✔              |
| torch.log2                                   | ✔         | ✘          | ✘              |
| torch.logical_not                            | ✔         | ✔          | ✘              |
| torch.logiacal_xor                           | ✔         | ✔          | ✘              |
| torch.mul                                    | ✔         | ✘          | ✔              |
| torch.mvlgamma                               | ✔         | ✘          | ✘              |
| torch.neg                                    | ✔         | ✘          | ✔              |
| torch.polygamma                              | ✔         | ✔          | ✘              |
| torch.pow                                    | ✔         | ✔          | ✔              |
| torch.real                                   | ✔         | ✔          | ✔              |
| torch.reciprocal                             | ✔         | ✔          | ✔              |
| torch.remainder                              | ✔         | ✘          | ✘              |
| torch.round                                  | ✔         | ✔          | ✔              |
| torch.rsqrt                                  | ✔         | ✔          | ✔              |
| torch.sign                                   | ✔         | ✔          | ✔              |
| torch.sin                                    | ✔         | ✔          | ✔              |
| torch.sinh                                   | ✔         | ✔          | ✔              |
| torch.sqrt                                   | ✔         | ✔          | ✔              |
| torch.tan                                    | ✔         | ✔          | ✔              |
| torch.trunc                                  | ✔         | ✘          | ✘              |
| torch.argmax                                 | ✔         | ✔          | ✔              |
| torch.argmin                                 | ✔         | ✔          | ✔              |
| torch.dist                                   | ✔         | ✘          | ✘              |
| torch.logsumexp                              | ✔         | ✘          | ✔              |
| torch.mean                                   | ✔         | ✔          | ✔              |
| torch.median                                 | ✔         | ✘          | ✘              |
| torch.mode                                   | ✔         | ✘          | ✘              |
| torch.norm                                   | ✔         | ✔          | ✔              |
| torch.prod                                   | ✔         | ✔          | ✔              |
| torch.std                                    | ✔         | ✔          | ✘              |
| torch.std_mean                               | ✔         | ✘          | ✘              |
| torch.unique                                 | ✔         | ✔          | ✘              |
| torch.unique_consecutive                     | ✔         | ✘          | ✘              |
| torch.var                                    | ✔         | ✔          | ✘              |
| torch.var_mean                               | ✔         | ✘          | ✘              |
| torch.allclose                               | ✔         | ✘          | ✘              |
| torch.argsort                                | ✔         | ✔          | ✘              |
| torch.eq                                     | ✔         | ✘          | ✔              |
| torch.equal                                  | ✔         | ✔          | ✔              |
| torch.ge                                     | ✔         | ✘          | ✘              |
| torch.gt                                     | ✔         | ✘          | ✔              |
| torch.isfinite                               | ✔         | ✘          | ✔              |
| torch.isinf                                  | ✔         | ✘          | ✔              |
| torch.isnan                                  | ✔         | ✘          | ✔              |
| torch.kthvalue                               | ✔         | ✘          | ✘              |
| torch.le                                     | ✔         | ✘          | ✘              |
| torch.lt                                     | ✔         | ✘          | ✘              |
| torch.max                                    | ✔         | ✔          | ✔              |
| torch.min                                    | ✔         | ✔          | ✔              |
| torch.ne                                     | ✔         | ✘          | ✘              |
| torch.sort                                   | ✔         | ✔          | ✘              |
| torch.topk                                   | ✔         | ✘          | ✔              |
| torch.fft                                    | ✔         | ✔          | ✔              |
| torch.ifft                                   | ✔         | ✔          | ✔              |
| torch.rfft                                   | ✔         | ✔          | ✔              |
| torch.irfft                                  | ✔         | ✔          | ✔              |
| torch.stft                                   | ✔         | ✔          | ✔              |
| torch.bartlett_window                        | ✔         | ✘          | ✘              |
| torch.blackman_window                        | ✔         | ✘          | ✘              |
| torch.hamming_window                         | ✔         | ✔          | ✘              |
| torch.hann_window                            | ✔         | ✔          | ✘              |
| torch.bincount                               | ✔         | ✔          | ✘              |
| torch.broadcast_tensors                      | ✔         | ✘          | ✘              |
| torch.cartesian_prod                         | ✔         | ✘          | ✘              |
| torch.cdist                                  | ✔         | ✘          | ✘              |
| torch.combinations                           | ✔         | ✘          | ✘              |
| torch.cross                                  | ✔         | ✔          | ✘              |
| torch.cumprod                                | ✔         | ✔          | ✘              |
| torch.cumsum                                 | ✔         | ✔          | ✔              |
| torch.diag                                   | ✔         | ✔          | ✘              |
| torch.diag_embed                             | ✔         | ✘          | ✘              |
| torch.diagflat                               | ✔         | ✘          | ✘              |
| torch.diagonal                               | ✔         | ✘          | ✘              |
| torch.einsum                                 | ✔         | ✔          | ✘              |
| torch.flatten                                | ✔         | ✔          | ✔              |
| torch.flip                                   | ✔         | ✘          | ✘              |
| torch.rot90                                  | ✔         | ✔          | ✘              |
| torch.histc                                  | ✔         | ✘          | ✘              |
| torch.meshgrid                               | ✔         | ✔          | ✘              |
| torch.renorm                                 | ✔         | ✘          | ✘              |
| torch.repeat_interleave                      | ✔         | ✘          | ✘              |
| torch.roll                                   | ✔         | ✔          | ✘              |
| torch.tesnordot                              | ✔         | ✔          | ✘              |
| torch.tril                                   | ✔         | ✘          | ✘              |
| torch.tril_indices                           | ✔         | ✘          | ✘              |
| torch.triu                                   | ✔         | ✘          | ✘              |
| torch.triu_indices                           | ✔         | ✘          | ✘              |
| torch.addbmm                                 | ✔         | ✘          | ✘              |
| torch.addmv                                  | ✔         | ✘          | ✘              |
| torch.addr                                   | ✔         | ✘          | ✘              |
| torch.baddbmm                                | ✔         | ✘          | ✘              |
| torch.bmm                                    | ✔         | ✘          | ✘              |
| torch.chain_matmul                           | ✔         | ✘          | ✘              |
| torch.cholesky                               | ✔         | ✔          | ✘              |
| torch.cholesky_inverse                       | ✔         | ✘          | ✘              |
| torch.cholesky_solve                         | ✔         | ✔          | ✘              |
| torch.dot                                    | ✔         | ✔          | ✔              |
| torch.eig                                    | ✔         | ✔          | ✘              |
| torch.geqrf                                  | ✔         | ✘          | ✘              |
| torch.ger                                    | ✔         | ✘          | ✘              |
| torch.inverse                                | ✔         | ✘          | ✘              |
| torch.det                                    | ✔         | ✔          | ✘              |
| torch.logdet                                 | ✔         | ✔          | ✘              |
| torch.slogdet                                | ✔         | ✔          | ✘              |
| torch.lstsq                                  | ✔         | ✔          | ✘              |
| torch.lu                                     | ✔         | ✔          | ✘              |
| torch.lu_solve                               | ✔         | ✔          | ✘              |
| torch.lu_unpack                              | ✔         | ✘          | ✘              |
| torch.matmul                                 | ✔         | ✔          | ✔              |
| torch.matrix_power                           | ✔         | ✘          | ✘              |
| torch.matrix_rank                            | ✔         | ✔          | ✘              |
| torch.mv                                     | ✔         | ✘          | ✘              |
| torch.orgqr                                  | ✔         | ✘          | ✘              |
| torch.ormqr                                  | ✔         | ✘          | ✘              |
| torch.pinverse                               | ✔         | ✘          | ✘              |
| torch.qr                                     | ✔         | ✔          | ✔              |
| torch.solve                                  | ✔         | ✔          | ✘              |
| torch.svd                                    | ✔         | ✔          | ✘              |
| torch.symeig                                 | ✔         | ✘          | ✘              |
| torch.trapz                                  | ✔         | ✘          | ✘              |
| torch.triangular_solve                       | ✔         | ✔          | ✘              |
| torch.compiled_with_cxx11_abi                | ✔         | ✘          | ✘              |
| torch.result_type                            | ✔         | ✘          | ✘              |
| torch.can_cast                               | ✔         | ✘          | ✘              |
| torch.promote_types                          | ✔         | ✘          | ✘              |
| torch.div(Truediv)                           | ✔         | ✔(truediv) | ✔  (div)       |
| torch.div(rtruediv)                          | ✔         | ✘          | ✔  (div)       |
| torch.sub                                    | ✔         | ✘          | ✔              |
| __rsub__                                     | ✔         | ✘          | ✔  (sub)       |
| copy                                         | ✔         | ✔          | ✔  (clone)     |
| float                                        | ✔         | ✘          | ✔  (cast)      |
| select                                       | ✔         | ✘          | ✔  (gather)    |
